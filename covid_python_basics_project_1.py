# -*- coding: utf-8 -*-
"""COVID_python_basics_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOfLsyabBVQDKR5KTAiwL3sfBHDbxvzQ

Que: Does ML model draw inferences from categorical data also, like gender? If then then categorical data also should be correctly imputed.

*Importing required modules.*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""*Mounting G-drive. Or comment it if connected.*"""

#from google.colab import drive
#drive.mount('/content/drive')

"""# **Analysis of confirmed_case_data**

*Importing datasets. Provide the correct path file as per your system.*
"""

confirmed_case_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_confirmed_v1_lyst1747728690432.csv')

"""*Take a glance at dataset one by one.*"""

print(f'The shape of the confirmed_cases_data set is {confirmed_case_data.shape}', end = '\n\n')
confirmed_case_data.head(3)

"""*Refactoring the column positions and names for easyness.*

*   *Swapping Country & province column; country being the broader category.*
*   *Full name of Lat & Long.*
*   *Formatting date coluns as per IST standards.*
"""

#converting dataframe columns to list columns to do swap
col_list = list(confirmed_case_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
confirmed_case_data = confirmed_case_data[col_list]

confirmed_case_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = confirmed_case_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
confirmed_case_data.columns = list(confirmed_case_data.columns[:4]) + list(new_date_columns)

confirmed_case_data.head(3)

"""*Now lets see dtypes of each column.*"""

confirmed_case_data.info()  # usually used to see dtypes. Pandas is truncating the output when there are too many columns.

confirmed_case_data.dtypes
# Date dtypes shuold be int64 as date is never in decimal.
# But if there are any missing values in date column, then its stype would be float64 (since pandas uses NaN for missing values, which is a float).

"""*From above we can see dtypes of Country, Province, Latitude and Longitude are fine.*

Let's check all Date column dtypes.
"""

print( * (col for col in confirmed_case_data.columns[4:] if 'int' not in str(confirmed_case_data[col].dtype)))
# unpacking operator * used to print the column names, otherwise it would print generator object.

# date_columns = confirmed_case_data.columns[4:]
# confirmed_case_data[date_columns] = confirmed_case_data[date_columns].astype('int64')

"""*Handling missing values*"""

# confirmed_case_data.isnull().sum() will give truncated result.
print(confirmed_case_data.isnull().any().sum()) # returns series with column name as index and value is the count of missing values in that column.

"""`// Don't forget to do isna() check at last`"""

# printing column names containing NaN values.
null_col = confirmed_case_data.isnull().sum()
print(null_col [null_col>0].to_string()) # to print where null values is more than 0.
# printing a pandas Series prints the dtype also at the end of the Series output (e.g., dtype: int64)
# Using to_string() converts series into its string representation for display purposes.

"""Above we have checked for NaN values. Note that it doesn't check for 0 values.

**Imputing Province missing values:**

*What to impute for Province ?*


*   From below we can see that 1 country has N provinces but the province is not repeated. Meaning each row is unique for a particular geolocation. So, we cannot use mode or median for imputation because it will lead to duplicate location. Hence we will impute the missing province values with 'Unknown' or could use the Country name itself in province.
{for those whose latit & longi are given, we could search the province from web, but not done as we have their co-ordinates anyway and province is just a categorical data}
"""

# Cross check if 1 country has many provinces.
duplicate_countries = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Country/Region'])]
print(duplicate_countries.shape, end = '\n\n')
duplicate_countries

# Checking if there are repeated Country-Province pair.
duplicate_country_province = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Country/Region', 'Province/State'])]
duplicate_country_province

"""Imputing 'All Provinces' value for missing province."""

confirmed_case_data['Province/State'] = confirmed_case_data['Province/State'].fillna('All Provinces')
confirmed_case_data.head(3)

"""Cross checking if it is updated"""

null_col = confirmed_case_data.isnull().sum()
print(null_col [null_col>0].to_string())
# So it is only Latitude and Longitude where values are NaN.

"""**Imputing missing values of Latitudes & Longitudes:**

*It is ideal that since dataset is geolocation specific, latitudes and longitudes should also be not repeated. But lets take a look.*
"""

duplicate_lat_long = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Latitude', 'Longitude'])]
duplicate_lat_long

"""*   From above we can see latitudes & longitudes are not repeated.
*   There are some (0,0) values which is humorously referred to as "Null Island" in the Atlantic Ocean where there is is no land, which also needs to be imputed.

*Lets see what all rows has NaN or 0 as Latitude or Longitudes.*
"""

missing_coords = confirmed_case_data[
    confirmed_case_data['Latitude'].isnull() |
    (confirmed_case_data['Latitude'] == 0) |
    confirmed_case_data['Longitude'].isnull() |
    (confirmed_case_data['Longitude'] == 0)
]
print(missing_coords.to_string())

for country in ['Canada', 'China', 'Diamond Princess', 'MS Zaandam']:
  print(country, confirmed_case_data.loc[confirmed_case_data['Country/Region'] == country].shape)

"""***What to impute in Latitudes and Longitudes?***

Latitude and longitudes should be treated as categorical or descrete or continous data ?

*   We cannot use mode or median, as it will lead to duplicate coordinates and may not represent that specific province of that country.

*   Observing filled data we saw coordinates represent the approximate central points of the capital cities of those provinces. For correct country and province, we can find the coordinates from internet. But there could be N such rows in large dataset. We will impute with the mean of that country's provinces coordinates. Logically coordinates are categorical data as here they are representing unique location, but given as continous numerical data.

 Here Diamond & Grand Princess are cruise ships quaratined during covid. And repatriated represents returned citizens, not a location. But dataset can cotain more such rows with filled Coordinates which we have not figured out. That's why we imputed these rows with mean of that country.

*   Diamond cruise and MS Zaandam in country column are cruise ship with province Unknown. And there is also 1 entry of them so cannot impute them with mean also. For not to loose data, we imputed them with 180 (which is pacific ocean loc) and document the value as special case.

Imputing Latitude and Longitude:
"""

for country in ['Canada', 'China']:
    country_specific_df = confirmed_case_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = confirmed_case_data.loc[country_specific_df & (confirmed_case_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = confirmed_case_data.loc[country_specific_df & (confirmed_case_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (confirmed_case_data['Latitude'].isnull() | (confirmed_case_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (confirmed_case_data['Longitude'].isnull() | (confirmed_case_data['Longitude'] == 0))
    confirmed_case_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    confirmed_case_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

"""Cross check if NaN is updated with Mean."""

confirmed_case_data.loc[[41, 42, 52, 58]]

# to update specific latitude and longitude
confirmed_case_data.loc[105, ['Latitude', 'Longitude']] = [180, -180]
confirmed_case_data.loc[173, ['Latitude', 'Longitude']] = [180, -180]
confirmed_case_data.loc[[105, 173]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""Cross check if any zero or NaN coordinates"""

missing_coords = confirmed_case_data[
    confirmed_case_data['Latitude'].isnull() |
    (confirmed_case_data['Latitude'] == 0) |
    confirmed_case_data['Longitude'].isnull() |
    (confirmed_case_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.")
else:
    print(missing_coords.to_string())

"""# **Analysis of death_data**

*Loading of dataset*
"""

death_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_deaths_v1_lyst1747728711771.csv')

"""*Data at glance*"""

print(f'The shape of the death_data set is {death_data.shape}', end = '\n\n')
death_data.head(3)

"""*Changed column names for easy understanding.*"""

# Reset the column names using the first row
death_data.columns = death_data.iloc[0]
# Drop the first row since it's now the header
death_data = death_data[1:].reset_index(drop=True) # drop true means old index is discarded and not added as a new column in your DataFram

#converting dataframe columns to list columns to do swap
col_list = list(death_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
death_data = death_data[col_list]

death_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = death_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
death_data.columns = list(death_data.columns[:4]) + list(new_date_columns)

death_data.head(3)

"""*Correcting dtype of columns*"""

death_data.dtypes

"""*Converting latitude and longitude dtype to float*"""

death_data['Longitude'] = death_data['Longitude'].astype('float64')
death_data['Latitude'] = death_data['Latitude'].astype('float64')

"""*Checking if any date column has dtype other than object*"""

print( * (col for col in death_data.columns[4:] if 'object' not in str(death_data[col].dtype)))

"""*Converting date column from object to int64*"""

# date_columns = death_data.columns[4:]
# death_data[date_columns] = death_data[date_columns].astype('int64')  # GIVES ValueError: cannot convert float NaN to integer
# death_data.dtypes

# we cannot change dtype of Date columns because one of the date column has NaN value. Nan is a float value, not int.
# so before converting dtype, we need to change NaN (float) with any int value. Then we can convert dtype to int.

"""*Columns with NaN values*"""

print(f' Number of Columns containing NaN values = {death_data.isnull().any().sum()}', end = '\n\n')

null_col = death_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""*Imputing provinces with value 'All Provinces'*"""

death_data['Province/State'] = death_data['Province/State'].fillna('All Provinces')
death_data.head(3)

"""Checking which rows has null or 0 latitudes and longitudes"""

missing_coords = death_data[
    death_data['Latitude'].isnull() |
    (death_data['Latitude'] == 0) |
    death_data['Longitude'].isnull() |
    (death_data['Longitude'] == 0)
]
print(missing_coords.to_string())

"""*We can see the situation is same as first dataset.*

*Imputing Latitudes & Longitudes.*
"""

for country in ['Canada', 'China']:
    country_specific_df = death_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = death_data.loc[country_specific_df & (death_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = death_data.loc[country_specific_df & (death_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (death_data['Latitude'].isnull() | (death_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (death_data['Longitude'].isnull() | (death_data['Longitude'] == 0))
    death_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    death_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

# to update specific latitude and longitude
death_data.loc[105, ['Latitude', 'Longitude']] = [180, -180]
death_data.loc[173, ['Latitude', 'Longitude']] = [180, -180]

missing_coords = death_data[
    death_data['Latitude'].isnull() |
    (death_data['Latitude'] == 0) |
    death_data['Longitude'].isnull() |
    (death_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.")
else:
    print(missing_coords.to_string())

death_data.loc[[41, 42, 52, 88, 105, 173]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""*Impting column '20/Apr/2020' Nan value.*

Lets see how many rows of column '20/Apr/2025' has NaN value.
"""

nan_rows = death_data[death_data['20/Apr/2020'].isnull()] # returns dataframe of all rows with NaN in column 20/Apr/2020
# print(nan_rows.to_string()) # -> df
print(nan_rows.index, end = '\n\n') # returns indexes of rows having NaN values -> Index (labels to access values in series or df). print(nan_rows.index[0])
death_data.loc[nan_rows.index, '20/Apr/2020'] # .loc[row label, column lablel] -> series (1D labelled array and that single column index will be 0)

"""*We can impute it with previous or forward day data, but lets see both days data first.*"""

previous_day_death = int(death_data.loc[nan_rows.index, '19/Apr/2020'].values[0])
one_day_after_death = int(death_data.loc[nan_rows.index, '21/Apr/2020'].values[0])

print(f'Previous day death = {previous_day_death}')
print(f'One day after death = {one_day_after_death}')
print(f'difference = {one_day_after_death - previous_day_death}')

"""Hence we will impute '20/Apr/2025' deaths with mean of these two days as the death_data is a cummulative data."""

death_data.loc[nan_rows.index, '20/Apr/2020'] = (previous_day_death + one_day_after_death)/2
death_data.loc[nan_rows.index, '20/Apr/2020']

print(f' Number of Columns containing NaN values = {death_data.isnull().any().sum()}', end = '\n\n')

null_col = death_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""Now we can convert dtype of date columns from Object to int"""

date_columns = death_data.columns[4:]
death_data[date_columns] = death_data[date_columns].astype('int64')

death_data.info()

"""# **Analysis of recovered_case_data**"""

recovered_case_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_recovered_v1_lyst1747728719904.csv')

print(f'The shape of the confirmed_cases_data set is {recovered_case_data.shape}', end = '\n\n')
recovered_case_data.head(3)

"""*We can see it has fomat anamolies similar to dataset 2.*

*So, perform all similar formatting options.*
"""

# Reset the column names using the first row
recovered_case_data.columns = recovered_case_data.iloc[0] # but we have not dropped the row1 which is column nmaes still.
# Drop the first row since it's now the header
recovered_case_data = recovered_case_data.drop(0).reset_index(drop=True) # reset index because you dropped first row. So, now index will be from 2.
#drop true means old index is removed and not added as a new column in your DataFram. Basically lost.

#converting dataframe columns to list columns to do swap
col_list = list(recovered_case_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
recovered_case_data = recovered_case_data[col_list]

recovered_case_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = recovered_case_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
recovered_case_data.columns = list(recovered_case_data.columns[:4]) + list(new_date_columns)

recovered_case_data.head(3)

"""*Dtypes Correction*"""

pd.DataFrame(recovered_case_data.dtypes).T

"""*Converting latitude and longitude dtype to float*"""

recovered_case_data['Longitude'] = recovered_case_data['Longitude'].astype('float64')
recovered_case_data['Latitude'] = recovered_case_data['Latitude'].astype('float64')

"""*Check if date columns have dtype other than object*"""

print( * (col for col in recovered_case_data.columns[4:] if 'object' not in str(recovered_case_data[col].dtype)))

"""since all date columns are object, they can have Nan values which can result in Valueerror in conversion from object dtype to int64 dtype.

So, we will first remove Nan then only convert its dtype.
"""

print(f' Number of Columns containing NaN values = {recovered_case_data.isnull().any().sum()}', end = '\n\n')

null_col = recovered_case_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""*Imputing provinces with value 'All Provinces'*"""

recovered_case_data['Province/State'] = recovered_case_data['Province/State'].fillna('All Provinces')
recovered_case_data.head(3)

"""*Checking which rows has null or 0 latitudes and longitudes*"""

missing_coords = recovered_case_data[
    recovered_case_data['Latitude'].isnull() |
    (recovered_case_data['Latitude'] == 0) |
    recovered_case_data['Longitude'].isnull() |
    (recovered_case_data['Longitude'] == 0)
]
print(missing_coords.to_string())

"""We can see the situation is same as first dataset but only 3 rows this time.

*Imputing Latitudes & Longitudes.*
"""

for country in ['China']:
    country_specific_df = recovered_case_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = recovered_case_data.loc[country_specific_df & (recovered_case_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = recovered_case_data.loc[country_specific_df & (recovered_case_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (recovered_case_data['Latitude'].isnull() | (recovered_case_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (recovered_case_data['Longitude'].isnull() | (recovered_case_data['Longitude'] == 0))
    recovered_case_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    recovered_case_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

# to update specific latitude and longitude
recovered_case_data.loc[90, ['Latitude', 'Longitude']] = [180, -180]
recovered_case_data.loc[158, ['Latitude', 'Longitude']] = [180, -180]

missing_coords = recovered_case_data[
    recovered_case_data['Latitude'].isnull() |
    (recovered_case_data['Latitude'] == 0) |
    recovered_case_data['Longitude'].isnull() |
    (recovered_case_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.", end = '\n\n')
else:
    print(missing_coords.to_string(), end = '\n\n')

recovered_case_data.loc[[73,90,158]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""*Imputing column '20/Apr/2020' Nan value.*

Lets see how many rows of column '20/Apr/2025' has NaN value.
"""

nan_rows = recovered_case_data[recovered_case_data['20/Apr/2020'].isnull()] # returns dataframe of all rows with NaN in column 20/Apr/2020
# print(nan_rows.to_string()) # -> df
print(nan_rows.index, end = '\n\n') # returns indexes of rows having NaN values -> Index (labels to access values in series or df). print(nan_rows.index[0])
recovered_case_data.loc[nan_rows.index, '20/Apr/2020'] # .loc[row label, column lablel] -> series (1D labelled array and that single column index will be 0)

"""We can impute it with previous or forward day data, but lets see both days data first."""

previous_day_death = int(recovered_case_data.loc[nan_rows.index, '19/Apr/2020'].values[0])
one_day_after_death = int(recovered_case_data.loc[nan_rows.index, '21/Apr/2020'].values[0])

print(f'Previous day death = {previous_day_death}')
print(f'One day after death = {one_day_after_death}')
print(f'difference = {one_day_after_death - previous_day_death}')

"""Hence we will impute '20/Apr/2025' deaths with mean of these two days as the death_data is a cummulative data."""

recovered_case_data.loc[nan_rows.index, '20/Apr/2020'] = (previous_day_death + one_day_after_death)/2
recovered_case_data.loc[nan_rows.index, '20/Apr/2020']

"""`mean of the previous and next day's values—also known as interpolation.`
`recovered_case_data['20/Apr/2025'].interpolate(method='linear', inplace=True)`
"""

print(f' Number of Columns containing NaN values = {recovered_case_data.isnull().any().sum()}', end = '\n\n')

null_col = recovered_case_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""Now we can convert dtype of date columns to int"""

date_columns = recovered_case_data.columns[4:]
recovered_case_data[date_columns] = recovered_case_data[date_columns].astype('int64')

recovered_case_data.info()

"""# ***Questions & Data Analysis***
---

Q1: ***How do you load the COVID-19 datasets for confirmed cases, deaths, and recoveries into Python using Pandas?***

Ans: Using readcsv() function of Panda'ss module, so that we can use pandas functionalities on our data set.

Q2.1: After loading the datasets, what is the structure of each dataset in terms of rows, columns, and data types?

Ans:
*   confirmed_case_data = (276, 498); {COuntry: Object, Province: Object, Latitude: Float, Longitude: FLoat, Date*: int64}
*   death_data = (277, 498); { col* : Object} later traformed to {Country: Object, Province: Object, Latitude: Float, Longitude: Float, Date*: int64}
*   recovered_case_data = (262, 498); { col* : Object} later traformed to {Country: Object, Province: Object, Latitude: Float, Longitude: Float, Date*: int64}

Q3.1: Identify these missing values and replace them using a suitable imputation method, such as forward filling, for time-series data.

Ans:
*   Provinces column missing values are imputed with 'All Provinces' value.
*   Latitude and Longitude values are imputed with mean of its country's values. And those for which there was single entry, (180,-180) is taken as special case & documented.

Q4.1: Replace blank values in the province column with "All Provinces."

And: Done

Q2.2: Generate plots of confirmed cases over time for the top countries.

Ans: The data is cummulative, so last day will give us total confirmed cases.
"""

country_based_data = confirmed_case_data.groupby('Country/Region').sum()
last_day_cases = country_based_data.loc[: , country_based_data.columns[-1]] # row,column
top_cases = last_day_cases.sort_values(ascending = False).head(10)
top_cases

# lineplot has categorical x axis. Trends over continuous x-axis; less intuitive for categories.
fig1 = plt.figure(figsize=(8, 3.5))
ax = sns.lineplot(data = top_cases, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
plt.xlabel('Country')
plt.ylabel('Confirmed Cases')
plt.title('Top 10 Countries with Highest Confirmed Cases')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
   #"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.show()
plt.close()

#Comparing discrete categories (like countries)
fig2 = plt.figure(figsize=(8,3.5))
ax = sns.barplot(x = top_cases.index, y = top_cases.values, palette='Blues')
plt.xlabel('Country')
plt.ylabel('Confirmed Cases')
plt.title('Top 10 Countries with Highest Confirmed Cases')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Q2.3: Generate plots of confirmed cases over time for China."""

china_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'China']
china_cases_series = china_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
china_cases_series # series indexed by dates. need to covert to dataframe for transpose

# converting china_cases_series.index to datetime format
china_cases_series.index = pd.to_datetime(china_cases_series.index)

# Now group by month and sum values
china_monthly = china_cases_series.groupby(china_cases_series.index.strftime("%b/%Y")).sum()
# this sorts grouped series in alphabetical order (e.g., "Apr" comes before "Aug" and "Dec") instead of chronological order.
# to sort inchronological order we need to convert back to %m%Y and then sort and then convert back to %b%Y.

#Converting to %m -> sorting -> back to %b

# Convert index to datetime, then format it as 'MM/YYYY'
china_monthly.index = pd.to_datetime(china_monthly.index, format='%b/%Y')

# Sort the index
china_monthly = china_monthly.sort_index()

# Convert index back to 'Mon/YYYY'
china_monthly.index = pd.to_datetime(china_monthly.index, format='%m/%Y').strftime('%b/%Y')

china_monthly

plt.clf()
  # lineplot has categorical x axis.
fig3 = plt.figure(figsize=(10,5))
axo = sns.lineplot(data = china_monthly, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
axo.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
#"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.xlabel('Days')
plt.ylabel('Confirmed Cases per month')
plt.title('China cases per Month')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
plt.savefig('china_cases_per_day.png')
plt.show()

# if you are giving two times sns.lineplot to use data in one and attributes in another, it will lead next grapgh to catch first graph data.

"""Q5.1: Analyze the peak number of daily new cases in Germany, France, and Italy. Which country experienced the highest single-day surge, and when did it occur?

Ans: Approach: Group data for these 3 countries. Then find the maximum different between consecutive days. Then plot graph of all 3 countries.
"""

germany_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Germany']
germany_daily_series = germany_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
germany_daily_series

france_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'France']
france_daily_series = france_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
france_daily_series

italy_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Italy']
italy_daily_series = italy_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
italy_daily_series

# italy_daily_series is a series. Accessing using loc, loc needs actual label which is date here. But iloc works with default index which are 0,1, etc.
# italy_daily_series.index[0] will give 0th row label.

series_dict = {
    "Germany": germany_daily_series,
    "France": france_daily_series,
    "Italy": italy_daily_series
}

max_diff_dict = {}

for name, seri in series_dict.items():
    max_diff = 0
    cases_surge_date = None

    for idx in range(1, len(seri)):  # Iterate through entire Series
        diff = seri.iloc[idx] - seri.iloc[idx - 1]
        if diff > max_diff:
            max_diff = diff
            cases_surge_date = seri.index[idx]

    print(f"{name}: Greatest difference is {max_diff} on {cases_surge_date}")
    max_diff_dict[name] = (int(max_diff), cases_surge_date)

print('\n the max_diff_dict is')
max_diff_dict

# Extract labels and values
labels = list(max_diff_dict.keys())  # Country names
proportion = [cases[0] for cases in max_diff_dict.values()]  # Case numbers
dates = [date[1] for date in max_diff_dict.values()]  # Surge dates

print(labels)
print(proportion)
print(dates)

# Custom function to display percentage, value, and date
# Custom function to display proportion and date
def autopct_format(all_values, all_dates):
    def format_func(pct):
        total = sum(all_values)
        absolute = int(round(pct * total / 100.0))  # Convert percentage to absolute value
        index = next((i for i, v in enumerate(all_values) if v == absolute), None)  # Find correct index
        if index is not None:
            return f"{all_values[index]}\n{all_dates[index]}"  # Display case numbers & date
        return ""

    return format_func

# Create Pie Chart
fig4 = plt.figure(figsize=(5,5))
plt.pie(proportion, labels=labels, autopct=autopct_format(proportion, dates), startangle=140, colors=['lightblue', 'lightcoral', 'lightgreen'])
plt.title("COVID-19 Case Surge Distribution")
plt.show()

"""Q6.2: What is the total number of deaths reported per country up to the current date?"""

country_based_death_data = death_data.groupby('Country/Region').sum()
last_day_deaths = country_based_death_data.iloc[:, -1]  # Use iloc for last column selection
top_deaths_countries = last_day_deaths.sort_values(ascending=False).head(10)
top_deaths_countries

#Comparing discrete categories (like countries)
fig5 = plt.figure(figsize=(8,3.5))
ax = sns.barplot(x = top_deaths_countries.index, y = top_deaths_countries.values, palette='Blues')
plt.xlabel('Country')
plt.ylabel('Total Death Cases')
plt.title('Top 10 Countries with Highest Death Cases')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Q6.3: What are the top 5 countries with the highest average daily deaths?"""

country_based_death_data = death_data.groupby('Country/Region').sum()
last_day_deaths = country_based_death_data.iloc[:, -1]  # Use iloc for last column selection
top_deaths_countries = last_day_deaths.sort_values(ascending=False).head(5)

date_columns = death_data.columns[4:]

top_avg_deaths_countries = top_deaths_countries / (len(date_columns))
top_avg_deaths_countries

"""Q5.3: What is the distribution of death rates (deaths/confirmed cases) among provinces in Canada? Identify the province with the highest and lowest death
rate as of the latest data point.

Ans: Approach:
`Confirmed case data -> pick only Canada as country with its provinces`

`Death case data ->   pick only Canada as country with its provinces`

`Do dividion for death rate ->  Confirmed case data / Death case data`

`Plot`
"""

canada_confirmed_case = confirmed_case_data[confirmed_case_data['Country/Region'] == "Canada"]
canada_confirmed_case_data = canada_confirmed_case.drop(columns=['Country/Region', 'Latitude', 'Longitude']).set_index('Province/State').iloc[:,-1]

canada_death_case = death_data[death_data['Country/Region'] == "Canada"]
canada_death_case_data = canada_death_case.drop(columns=['Country/Region', 'Latitude', 'Longitude']).set_index('Province/State').iloc[:,-1]
canada_death_case_data

canada_death_rate = (canada_death_case_data/canada_confirmed_case_data)*100
canada_death_rate

# it will inf only if 0/any means no case but died. Or 0/0 means no case no death.
# Impute inf values with 0
canada_death_rate = canada_death_rate.replace([np.inf, -np.inf], 0)
print(canada_death_rate)  #Ex For Alberta = 2214 deaths/226855cases = 0.97%

fig6 = plt.figure(figsize=(8, 6))
ax = sns.lineplot(data = canada_death_rate, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
plt.xlabel('Canada Provinces')
plt.ylabel('Death rates')
plt.title('Canada province death rates')
plt.xticks(rotation=90)
   # To avoid exponential notation on y-axis
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
   #"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
# Add number annotations at each marker
for i, value in enumerate(canada_death_rate):
    ax.text(i, value, f"{value:.2f}", fontsize=10, ha='center', va='bottom', color='black')
plt.show()
plt.close()

"""Q6.4: How have the total deaths evolved over time in the United States?

Ans: Actually it is asking trend of deaths in US overtime. So, we will plot for month wise deaths for US and see if the deaths increased or decresed (i.e., evolution)
"""

US_data = death_data[death_data['Country/Region'] == 'US']
US_cases_series = US_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
US_cases_series # series indexed by dates. need to covert to dataframe for transpose

# converting US_cases_series.index to datetime format
US_cases_series.index = pd.to_datetime(US_cases_series.index)

# Now group by month and sum values
US_monthly = US_cases_series.groupby(US_cases_series.index.strftime("%b/%Y")).sum()
# this sorts grouped series in alphabetical order (e.g., "Apr" comes before "Aug" and "Dec") instead of chronological order.
# to sort inchronological order we need to convert back to %m%Y and then sort and then convert back to %b%Y.

#Converting to %m -> sorting -> back to %b

# Convert index to datetime, then format it as 'MM/YYYY'
US_monthly.index = pd.to_datetime(US_monthly.index, format='%b/%Y')

# Sort the index
US_monthly = US_monthly.sort_index()

# Convert index back to 'Mon/YYYY'
US_monthly.index = pd.to_datetime(US_monthly.index, format='%m/%Y').strftime('%b/%Y')

US_monthly

plt.clf()
  # lineplot has categorical x axis.
fig7 = plt.figure(figsize=(10,5))
axo = sns.lineplot(data = US_monthly, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
axo.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
#"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.xlabel('Days')
plt.ylabel('Confirmed Cases per month')
plt.title('China cases per Month')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
plt.savefig('china_cases_per_day.png')
plt.show()

# if you are giving two times sns.lineplot to use data in one and attributes in another, it will lead next grapgh to catch first graph data.

"""Q5.2: Compare the recovery rates (recoveries/confirmed cases) between Canada and Australia as of December 31, 2020. Which country showed better management of the
pandemic according to this metric?
"""

#Canada positive data
canada_confirm_case_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Canada']
canada_confirm_case_data_series = canada_confirm_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
#sums column-wise means for each date column, it sums across all provinces in Canada.
canada_case_31Dec2020 = canada_confirm_case_data_series.loc['31/Dec/2020']
print(f'canada confirmed case on 31dec 2020 is {canada_case_31Dec2020}')

#Canada recovery data
canada_recovery_case_data = recovered_case_data[recovered_case_data['Country/Region'] == 'Canada']
canada_recovery_case_data_series = canada_recovery_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
canada_recovery_case_31Dec2020 = canada_recovery_case_data_series.loc['31/Dec/2020']
print(f'canada confirmed case on 31dec 2020 is {canada_recovery_case_31Dec2020}')

canada_recovery_rate = (canada_recovery_case_31Dec2020/canada_case_31Dec2020)*100
print(f'canada recovery rate is {canada_recovery_rate}')

#Australia positive data
australia_confirm_case_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Australia']
australia_confirm_case_data_series = australia_confirm_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
#sums column-wise means for each date column, it sums across all provinces in Australia.
australia_case_31Dec2020 = australia_confirm_case_data_series.loc['31/Dec/2020']
print(f'Australia confirmed case on 31dec 2020 is {australia_case_31Dec2020}')

#Australia recovery data
australia_recovery_case_data = recovered_case_data[recovered_case_data['Country/Region'] == 'Australia']
australia_recovery_case_data_series = australia_recovery_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
australia_recovery_case_31Dec2020 = australia_recovery_case_data_series.loc['31/Dec/2020']
print(f'Australia confirmed case on 31dec 2020 is {australia_recovery_case_31Dec2020}')

australia_recovery_rate = (australia_recovery_case_31Dec2020/australia_case_31Dec2020)*100
print(f'australia recovery rate is {australia_recovery_rate}')

#Comparing discrete categories (like countries)
fig8 = plt.figure(figsize=(2,4))
ax = sns.barplot(x = ['Canada', 'Australis'], y = [canada_recovery_rate, australia_recovery_rate], palette='Greens')
plt.xlabel('Country')
plt.ylabel('Recovery rate')
plt.title('Recovery rate on 31dec/2020')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Hence we can see Canada government managed well for the recovery from the pendamic for day 31 Dec 2020 from the given metrics.

Q6.1: Transform the 'deaths' dataset from wide format (where each column represents a date) to long format, where each row represents a single date,
ensuring that the date column is in datetime format. How would this transformation be executed?

Ans: We have country, province, long, lat as one unique row and then dates. This is wide form. We have to convert in long form means country, province, long, lat and date. So for each country, province, long, lat there will be 494 rows. ANd there are 277 rows so 277*494 total rows will come in long. We are not performing any column wise province sum here.
"""

deaths_data_long = pd.melt(
    death_data,
    id_vars=['Country/Region', 'Province/State', 'Latitude', 'Longitude'],
    var_name='Date',
    value_name='Deaths'
)
deaths_data_long['Date'] = pd.to_datetime(deaths_data_long['Date'], format='%d/%b/%Y')
deaths_data_long.shape

"""Question 7: Data Merging
Q7.1: How would you merge the transformed datasets of confirmed cases, deaths, and recoveries on the 'Country/Region' and 'Date' columns to create a
comprehensive view of the pandemic's impact?
Q7.2: Analyze the monthly sum of confirmed cases, deaths, and recoveries for countries to understand the progression of the pandemic.[From the merged
dataset]
Q7.3: Redo the analysis in Question 7.2 for the United States, Italy, and Brazil.

Question 8: Combined Data Analysis
Q8.1: For the combined dataset, identify the three countries with the highest average death rates (deaths/confirmed cases) throughout 2020. What might
this indicate about the pandemic's impact in these countries?
Q8.2: Using the merged dataset, compare the total number of recoveries to the total number of deaths in South Africa. What can this tell us about the
outcomes of COVID-19 cases in the country?
Q8.3: Analyze the ratio of recoveries to confirmed cases for the United States monthly from March 2020 to May 2021. Which month experienced the highest
recovery ratio, and what could be the potential reasons?

"""







